import numpy as np
from nltk import sent_tokenize

from ..AbstractEvaluationClasses import AbstractEvaluationReferenceFree


# Modified from https://github.com/maszhongming/UniEval
class UniEvalDialog(AbstractEvaluationReferenceFree):
    """
    UniEval for dialogues

    Pytorch Model Download is 3.13 GB

    Init takes up 3.6GB of GPU memory
    Evaluate increases to 4GB
    """

    def __init__(
        self,
        device="auto",
        max_length=1024,
        cache_dir=None,  # Model download location?
    ):
        """Set up evaluator for dialogues"""
        if device not in ["cuda", "cpu"]:  # auto
            device = "cuda" if torch.cuda.is_available() else "cpu"

        self.scorer = _UniEvaluator(
            model_name_or_path="MingZhong/unieval-dialog",
            max_length=max_length,
            device=device,
            cache_dir=cache_dir,
        )
        self.task = "dialogue"
        self.dimensions = [
            "naturalness",
            "coherence",
            "engagingness",
            "groundedness",
            "understandability",
        ]

    def evaluate(
        self,
        dialogue_history: list or str,
        response: str,
        return_all_scores=False,
        dims: list = None,
        overall=True,
        print_result=False,
        context=None,
    ):
        # data, dims=None, overall=True, print_result=False):
        """
        Get the scores of all the given dimensions

        :param dialogue_history: Dialogue history. A list of strings or a single string.
        :param response: output text generated by the models to be evaluated.

        :param return_all_scores: whether to output the scores of all the given
                dimensions or only the overall score (overrides overall=False).

        :param dims: A list of dimensions to be evaluated.
                If dims is None, default dimensions

        :param overall: indicates whether the overall score is to be calculated.
                Overall score can be customized to a combination of scores
                based on different dimensions. The default here is the average
                score of all the given dimensions.

        :param print_result: whether to print a table with the score breakdown.
        :param context: the context needed to evaluate several specific dimension.
        """
        if dims is None or dims == "default":
            dims = ["naturalness", "coherence", "understandability"]
            # No engagingness and groundedness as they use additional context

        # ------ Convert to data format for UniEval -------
        if isinstance(dialogue_history, list):
            # join with \n and add \n\n to the end
            dialogue_history = "\n".join(dialogue_history) + "\n\n"
        # For Format See:
        # https://github.com/maszhongming/UniEval/blob/509075cc87bb64f239180ece460025466b260383/utils.py#L3C6-L3C6
        data = {"source": dialogue_history, "system_output": response}
        if context is not None:
            data["context"] = context

        eval_scores = {}  # Store the scores of all the given dimensions

        assert isinstance(dims, list)
        eval_dims = dims

        for dim in eval_dims:
            # Calculate summation score for 'engagingness'
            if dim == "engagingness":
                src_list, output_list, context_list = [], [], []
                n_sents = []  # the number of sentences in each generated response

                source = data["source"]  # the dialogue history
                context = data["context"] if "context" in data else ""
                system_outputs = sent_tokenize(
                    data["system_output"], language="english"
                )
                n_sents.append(len(system_outputs))

                src_list.append(source)
                context_list.append(context)
                output_list.append(system_outputs)
                input_list = add_question(
                    dimension=dim,
                    output=output_list,
                    src=src_list,
                    context=context_list,
                    task=self.task,
                )
                sent_score = self.scorer.score(input_list)

                # Get the summation score for each sample
                start_idx = 0
                score = []
                for cur_n_sent in n_sents:
                    score.append(sum(sent_score[start_idx : start_idx + cur_n_sent]))
                    start_idx += cur_n_sent

            # Calculate turn-level score for other dimensions
            elif dim in [
                "naturalness",
                "coherence",
                "groundedness",
                "understandability",
            ]:
                src = data["source"]
                output = data["system_output"]
                context = data["context"] if "context" in data else ""

                input_list = add_question(
                    dimension=dim,
                    output=[output],  # Making it a list for the add_question function
                    src=[src],
                    context=[context],
                    task=self.task,
                )
                score = self.scorer.score(input_list)

            # Please customize other dimensions here for summarization
            else:
                raise NotImplementedError(
                    "The input format for this dimension is still undefined. \
                                           Please customize it first."
                )

            eval_scores[dim] = score[0]  # Only one sample

        # Customize your overall score here.
        if overall:
            eval_scores["overall"] = np.mean(list(eval_scores.values()))

        if print_result:
            print_scores(eval_scores)

        if return_all_scores:
            return eval_scores
        if "overall" in eval_scores:
            return eval_scores["overall"]
        # Return the average overall score. Might be a single dimension score.
        return np.mean(list(eval_scores.values()))


# ================== UTILS ==================
# The following code is unmodified from https://github.com/maszhongming/UniEval
# and is licensed under the MIT License.
#
# MIT License
#
# Copyright (c) 2022 Ming Zhong
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


def add_question(dimension, output, src=None, ref=None, context=None, task=None):
    """
    Add questions to generate input in Bool-QA format for UniEval.

    dimension: specific dimension to be evaluated
    src: source input for different NLG tasks. For example, source document for summarization
         and dialogue history for dialogue response generation.
    output: output text generated by the models
    ref: human-annotataed groundtruth
    context: the context needed to evaluate several specific dimension. For example,
             additional factual information when evaluating engagingness and groundedness in dialogues.
    """

    input_with_question = []
    for i in range(len(output)):
        # For summarization
        if task == "summarization":
            if dimension == "fluency":
                cur_input = (
                    "question: Is this a fluent paragraph? </s> paragraph: " + output[i]
                )
            elif dimension == "coherence":
                cur_input = (
                    "question: Is this a coherent summary to the document? </s> summary: "
                    + output[i]
                    + " </s> document: "
                    + src[i]
                )
            elif dimension == "consistency":
                cur_input = (
                    "question: Is this claim consistent with the document? </s> claim: "
                    + output[i]
                    + " </s> document: "
                    + src[i]
                )
            elif dimension == "relevance":
                cur_input = (
                    "question: Is this summary relevant to the reference? </s> summary: "
                    + output[i]
                    + " </s> reference: "
                    + ref[i]
                )
            else:
                raise NotImplementedError(
                    "The input format for this dimension is still undefined. Please customize it first."
                )
        # For dialogues
        elif task == "dialogue":
            if dimension == "naturalness":
                cur_input = (
                    "question: Is this a natural response in the dialogue? </s> response: "
                    + output[i]
                )
            elif dimension == "coherence":
                cur_input = (
                    "question: Is this a coherent response given the dialogue history? </s> response: "
                    + output[i]
                    + " </s> dialogue history: "
                    + src[i]
                )
            elif dimension == "engagingness":
                cur_input = (
                    "question: Is this an engaging and informative response according to the dialogue history and fact? </s> response: "
                    + output[i]
                    + " </s> dialogue history: "
                    + src[i]
                    + " </s> fact: "
                    + context[i]
                )
            elif dimension == "groundedness":
                cur_input = (
                    "question: Is this response consistent with knowledge in the fact? </s> response: "
                    + output[i]
                    + " </s> fact: "
                    + context[i]
                )
            elif dimension == "understandability":
                cur_input = (
                    "question: Is this an understandable response in the dialogue? </s> response: "
                    + output[i]
                )
            else:
                raise NotImplementedError(
                    "The input format for this dimension is still undefined. Please customize it first."
                )
        # For data-to-text
        elif task == "data2text":
            if dimension == "naturalness":
                cur_input = (
                    "question: Is this a fluent utterance? </s> utterance: " + output[i]
                )
            elif dimension == "informativeness":
                cur_input = (
                    "question: Is this sentence informative according to the reference? </s> sentence: "
                    + output[i]
                    + " </s> reference: "
                    + ref[i]
                )
            else:
                raise NotImplementedError(
                    "The input format for this dimension is still undefined. Please customize it first."
                )
        # For factual consistency detection
        elif task == "fact":
            if dimension == "consistency":
                cur_input = (
                    "question: Is this claim consistent with the document? </s> claim: "
                    + output[i]
                    + " </s> document: "
                    + src[i]
                )
            else:
                raise NotImplementedError(
                    "No other dimensions for the factual consistency detection task."
                )
        # For new customized tasks
        else:
            raise NotImplementedError(
                "Other tasks are not implemented, please customize specific tasks here."
            )
        input_with_question.append(cur_input)
    return input_with_question


def print_scores(scores):
    from prettytable import PrettyTable

    table = PrettyTable(["Dimensions", "Score"])
    print("\nEvaluation scores are shown below:")
    dims = list(scores[0].keys())
    for dim in dims:
        cur_score = 0
        for i in range(len(scores)):
            cur_score += scores[i][dim]
        table.add_row([dim, round(cur_score / len(scores), 6)])
    print(table)


# ================== END UTILS  ==================

# ==================   SCORER   ==================
import torch
import torch.nn as nn
from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM


class _UniEvaluator:
    def __init__(
        self, model_name_or_path, max_length=1024, device="cuda", cache_dir=None
    ):
        """Set up model"""
        self.device = device
        self.max_length = max_length

        self.config = AutoConfig.from_pretrained(
            model_name_or_path, cache_dir=cache_dir
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path, cache_dir=cache_dir
        )
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name_or_path, config=self.config, cache_dir=cache_dir
        )

        self.model.eval()
        self.model.to(device)

        self.softmax = nn.Softmax(dim=1)

        self.pos_id = self.tokenizer("Yes")["input_ids"][0]
        self.neg_id = self.tokenizer("No")["input_ids"][0]

    def score(self, inputs, batch_size=8, log=False):
        """
        Get scores for the given samples.
        final_score = postive_score / (postive_score + negative_score)
        """
        # The implementation of "forward" in T5 still requires decoder_input_ids.
        # Therefore, we construct a random one-word target sequence.
        # The content of the target has no effect on the final scores.
        tgts = ["No" for _ in range(len(inputs))]

        pos_score_list, neg_score_list = [], []
        for i in range(0, len(inputs), batch_size):
            src_list = inputs[i : i + batch_size]
            tgt_list = tgts[i : i + batch_size]
            try:
                with torch.no_grad():
                    encoded_src = self.tokenizer(
                        src_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                    )
                    encoded_tgt = self.tokenizer(
                        tgt_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                    )

                    src_tokens = encoded_src["input_ids"].to(self.device)
                    src_mask = encoded_src["attention_mask"].to(self.device)

                    tgt_tokens = (
                        encoded_tgt["input_ids"].to(self.device)[:, 0].unsqueeze(-1)
                    )

                    output = self.model(
                        input_ids=src_tokens, attention_mask=src_mask, labels=tgt_tokens
                    )
                    logits = output.logits.view(-1, self.model.config.vocab_size)

                    pos_score = self.softmax(logits)[:, self.pos_id]  # Yes
                    neg_score = self.softmax(logits)[:, self.neg_id]  # No

                    cur_pos_score = [x.item() for x in pos_score]
                    cur_neg_score = [x.item() for x in neg_score]
                    pos_score_list += cur_pos_score
                    neg_score_list += cur_neg_score

            except RuntimeError:
                print(f"source: {src_list}")
                print(f"target: {tgt_list}")
                exit(0)

        score_list = []
        for i in range(len(pos_score_list)):
            score_list.append(
                pos_score_list[i] / (pos_score_list[i] + neg_score_list[i])
            )

        return score_list


# ================== END SCORER ==================
