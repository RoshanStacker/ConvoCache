import logging
import math
import re
import time
from pathlib import Path

import backoff
import openai

from ..AbstractEvaluationClasses import AbstractEvaluationReferenceFree


class GEvalEvaluator(AbstractEvaluationReferenceFree):
    """
    G-EVAL for dialogues
    http://arxiv.org/abs/2303.16634

    Code Adapted from
    https://github.com/nlpyang/geval
    """

    prompt_folder = Path(__file__).parent / "prompts"

    prompt_files = {
        "coherence": prompt_folder / "coherence_custom_paper.txt",
    }
    dimension_ranges = {
        "coherence": (0, 5),
    }

    def __init__(self, openai_key: str = None, model: str = "gpt-4-0613"):
        # Test GPT keys
        if openai_key is None:
            # Load from environment variable
            from os import getenv

            openai_key = getenv("OPENAI_KEY")
            if openai_key is None:
                raise ValueError(
                    "OPENAI_KEY environment variable not set. "
                    "Please set it or pass it to the constructor."
                )
        openai.api_key = openai_key
        self.model = model

    def evaluate(
        self,
        dialogue_history: list[str] or str,
        response: str,
        dims: list[str] = None,
        custom_prompt: str = None,
        custom_score_range: tuple[float, float] = (0, 5),
        model: str = None,
    ) -> float:
        """
        Get the G-Eval score in the range of 0 to 1

        :param dialogue_history: Dialogue history. A list of strings or a single string.
        :param response: output text generated by the models to be evaluated.
        :param dims: List of dimensions to evaluate. If None, all dimensions are evaluated.
                To use only custom_prompt, set dims to an empty list.
                Options are:
                - "coherence"
        :param custom_prompt: Custom prompt to use as text. Will be used alongside the dims
                Include '{{Document}}' for Dialogue history and '{{Response}}' for response
                to be filled in from the dialogue_history and response parameters.
        :param custom_score_range: Custom score range for the custom prompt.
        :param model: OpenAI Model string to be used. Leave blank to use the self.model default
                that was initialised in the constructor.

        :return: Scores averaged and normalised to [0, 1]
        """
        if dims is None:
            # Evaluate all dimensions
            dims = list(self.prompt_files.keys())
        else:
            # Check if all dimensions are valid
            for dim in dims:
                if dim not in self.prompt_files:
                    raise ValueError(f"Invalid dimension {dim}.")

        # Format dialogue history into string
        if isinstance(dialogue_history, list):
            # join with \n and add \n\n to the end
            dialogue_history = "\n".join(dialogue_history) + "\n\n"

        # Prompts   {dimension: prompt}
        prompts: dict[str, str] = {
            dim: "" for dim in dims
        }  # Initialize an empty string for each dimension

        # Read prompt from file
        for dimension in dims:
            try:
                with open(self.prompt_files[dimension], "r") as file:
                    prompts[dimension] = file.read()
            except FileNotFoundError:
                del prompts[dimension]
                logging.warning(
                    f"Prompt file for dimension {dimension} not found. Skipping."
                )

        # Add custom prompt
        if custom_prompt is not None:
            prompts["custom"] = custom_prompt

        # Add dialogue and response to prompts
        for dimension in prompts:
            prompts[dimension] = (
                prompts[dimension]
                .replace("{{Document}}", dialogue_history)
                .replace("{{Response}}", response)
            )
            if re.search(r"\{\{.*?\}\}", prompts[dimension]):  # Find {{unfilled}}
                logging.critical(
                    f"Prompt for dimension {dimension} is not filled in. "
                    f"Cancel and check GEVAL.py. Continuing in 5 seconds."
                )
                time.sleep(5)

        # Evaluate each prompt
        scores = {}
        for dimension in prompts:
            scores[dimension] = self._evaluate_prompt(prompts[dimension], model)

            # Normalise
            if dimension in self.dimension_ranges:
                scores[dimension] = (
                    scores[dimension] - self.dimension_ranges[dimension][0]
                ) / (
                    self.dimension_ranges[dimension][1]
                    - self.dimension_ranges[dimension][0]
                )
        #
        # # Normalise custom score
        # if "custom" in scores:
        #     scores["custom"] = (scores["custom"] - custom_score_range[0]) / (
        #         custom_score_range[1] - custom_score_range[0]
        #     )

        if len(scores) == 0:
            logging.warning("No scores were calculated. Returning 0.")
            return 0
        if len(scores) == 1:
            return list(scores.values())[0]
        else:
            return sum(scores.values()) / len(scores)  # Average Score

    def _evaluate_prompt(
        self,
        prompt,
        model,
    ) -> float:
        """
        Evaluate a single prompt. Uses weighted LogProbs of top 5 tokens.
        :param prompt: Prompt to evaluate
        :param model: OpenAI Model string to be used. Leave blank to use the self.model default
                that was initialised in the constructor.
        :return: G-Eval score without normalisation
        """
        if model is None:
            model = self.model

        # Calculate time to prevent API limit of Requests Per Minute (RPM)
        start_time = time.time()
        _response = self._openai_ChatCompletion_backoff(
            model=model,
            messages=[{"role": "system", "content": prompt}],
            temperature=0,  # Using LogProbs not Sampling
            max_tokens=5,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=None,
            logprobs=True,
            top_logprobs=5,  # 5 is the max
            n=1,  # Using LogProbs not Sampling
        )
        score = self._get_score_from_logprobs(_response.choices[0])

        # Tier 2 API limit
        if "gpt-3.5" in model:
            rpm = 3000  # Actual RPM is 3500
        elif "gpt-4" in model and "vision-preview" not in model:
            rpm = 4000  # Actual RPM is 5000.
        else:
            rpm = 75
        time_taken = time.time() - start_time
        time_to_sleep = 60 / rpm - time_taken
        if time_to_sleep > 0:
            time.sleep(time_to_sleep)

        return score  # Average score not normalised

    @staticmethod
    @backoff.on_exception(
        backoff.expo,
        (
            openai.error.RateLimitError,
            openai.error.ServiceUnavailableError,
            openai.error.Timeout,
            openai.error.APIError
        ),
        max_time=60 * 60 * 24,
        max_tries=1000,
        logger=logging.getLogger(__name__),
        backoff_log_level=logging.WARNING,
    )
    def _openai_ChatCompletion_backoff(*args, **kwargs):
        return openai.ChatCompletion.create(*args, **kwargs)

    _count_non_number_tokens = 0

    @staticmethod
    def _get_score_from_logprobs(_response):
        log_probs = _response["logprobs"]["content"][0]["top_logprobs"]
        score = 0
        for token_prob in log_probs:
            # check if number
            try:
                number = float(token_prob["token"])
                if number == 0:
                    continue
                if number > 5:
                    continue
                log_prob = token_prob["logprob"]
                prob = math.exp(log_prob)
                score += number * prob
            except ValueError:
                GEvalEvaluator._count_non_number_tokens += 1
                if (
                    GEvalEvaluator._count_non_number_tokens > 1000000
                    or GEvalEvaluator._count_non_number_tokens
                    in [1, 10, 100, 1000, 10000, 100000, 1000000]
                ):
                    logging.warning(
                        f"Non-number token: {token_prob['token']}... "
                        f"{GEvalEvaluator._count_non_number_tokens} "
                        f"non-number tokens so far."
                    )
                continue
        if score < 0:
            # debug
            logging.warning(f"Score is negative: {score}")

            logging.warning(f"Logprobs: {log_probs}")
            score = 0
        return score

    @staticmethod
    def _parse_output(output) -> float:
        """Parse response for the numerical score"""
        matched = re.search("^ ?([\d\.]+)", output)
        if matched:
            try:
                score = float(matched.group(1))
            except:
                score = 0
        else:
            score = 0
        return score
